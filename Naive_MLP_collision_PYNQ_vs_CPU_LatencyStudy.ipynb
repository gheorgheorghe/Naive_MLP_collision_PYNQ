{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from utils import *\n",
    "from dataset import CollisionDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries needed to use the accelerator\n",
    "import pynq.lib.dma # For using the DMA\n",
    "from pynq import Xlnk # Used for allocating contiguous arrays\n",
    "import numpy as np # Xlnk uses numpy arrays\n",
    "from pynq import Overlay # Used to download the bitstream\n",
    "import struct\n",
    "from pynq import DefaultIP # Used for AXI-Lite class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = Overlay('/home/xilinx/Linear2x7_112420_2wayDMA_2/backward_lite_features.bit') # Download the bitstream onto the FPGA\n",
    "\n",
    "# In this accelerator, we are accelerating two kernels and each has its own DMA which are assigned here:\n",
    "dma1 = overlay.axi_dma_0 # Backward\n",
    "# dma2 = overlay.axi_dma_1 # Equation Matrix\n",
    "\n",
    "# Since this IP uses AXI-Lite for the output, we can associate that to a variable and then use our class defined above through \n",
    "# this\n",
    "# backward_ip = overlay.backward_lite_0\n",
    "\n",
    "xlnk = Xlnk() # Used for allocation\n",
    "# Allocating the contiguous arrays of a fixed size:\n",
    "in_stream = xlnk.cma_array(shape=(32*2+32*2+(32*7),1), dtype=np.float32)\n",
    "out_stream = xlnk.cma_array(shape=((7+1)*2), dtype=np.float32)\n",
    "\n",
    "in_buffer = xlnk.cma_array(shape=(32,2), dtype=np.float32)\n",
    "out_buffer = xlnk.cma_array(shape=(32,7), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(NaiveMLP, self).__init__()\n",
    "        self.hidden_dim = 128\n",
    "        self.fc_1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, self.hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.fc_out = nn.Linear(self.hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_1(x)\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs, optimizer, scheduler, criterion):\n",
    "    model.train()\n",
    "        \n",
    "    # Training the Model\n",
    "    min_test_dif = float('inf')\n",
    "    epoch_loss = []\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_loss = []\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # get the inputs\n",
    "            x = data['x']\n",
    "            y = data['y']\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            layer1 = net[0]\n",
    "            z = layer1(x)\n",
    "            y_hat = net(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            \n",
    "            \n",
    "            # Pre-processing for the other accelerator\n",
    "            batch_x_stream = x.t()\n",
    "            batch_x_stream = batch_x_stream.reshape(32*7, 1)\n",
    "            y_stream = y.t()\n",
    "            y_stream = y_stream.reshape(32 * 2, 1);\n",
    "            y_hat_stream = y_hat.t()\n",
    "            y_hat_stream = y_hat_stream.reshape(32 * 2, 1);\n",
    "            z_stream = z.t()\n",
    "            z_stream = z_stream.reshape(32*2,1)\n",
    "            \n",
    "            in_stream[:] = torch.cat(( y_stream.data, y_hat_stream.data, z_stream.data, batch_x_stream.data), 0).numpy()[:]\n",
    "            \n",
    "            t = time.time()\n",
    "            loss.backward()\n",
    "            pytorchLatency = time.time() - t\n",
    "\n",
    "            # Transfer data to the DMA\n",
    "            t = time.time()\n",
    "            dma1.sendchannel.transfer(in_stream)\n",
    "            dma1.recvchannel.transfer(out_stream)\n",
    "            dma1.sendchannel.wait()\n",
    "            dma1.recvchannel.wait()\n",
    "            PYNQLatency = time.time() - t\n",
    "            \n",
    "#             manualGrad = backward_manual(y_hat, y, z, x)\n",
    "\n",
    "            weight_grad = torch.reshape(torch.tensor(out_stream[0:14]), (2,7))\n",
    "            bias_grad = torch.squeeze(torch.tensor(out_stream[14:16]))\n",
    "            print()\n",
    "            print()\n",
    "            flag = True\n",
    "            for param in model.parameters():\n",
    "                if (flag):\n",
    "                    RMSE = np.sqrt(np.mean(np.square((weight_grad - param.grad).numpy())))\n",
    "                    print('PyTorch - PYNQ backprop RMSE for W:')\n",
    "                    print(RMSE)\n",
    "                    flag = False\n",
    "                else:\n",
    "                    RMSE = np.sqrt(np.mean(np.square((bias_grad - param.grad).numpy())))\n",
    "                    print('PyTorch - PYNQ backprop RMSE for b:')\n",
    "                    print(RMSE)\n",
    "                    flag = True\n",
    "                    \n",
    "            \n",
    "        \n",
    "            print()\n",
    "            print('Pytorch backprop latency:')\n",
    "            print(str(round(pytorchLatency,5)) + \" s\")\n",
    "            print('PYNQ backprop latency:')\n",
    "            print(str(round(PYNQLatency,5)) + \" s\")\n",
    "            print('Acceleration factor (CPU_Latency / PYNQ_Latency): ')\n",
    "            print(round(pytorchLatency / PYNQLatency,5))\n",
    "            print()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_loss.append(loss.item())\n",
    "\n",
    "        # Results every epoch\n",
    "        cur_epoch_loss = sum(batch_loss) / len(batch_loss)\n",
    "        \n",
    "        # Scheduler\n",
    "        scheduler.step(cur_epoch_loss)\n",
    "        \n",
    "        # Test the network\n",
    "        train_dif = test_model(model, train_loader, criterion)\n",
    "        test_dif = test_model(model, test_loader, criterion)\n",
    "        \n",
    "        # Print the result\n",
    "        print('Epoch: %d Train Loss: %f Train Dif: %f Test Dif: %f' \n",
    "              % (epoch, cur_epoch_loss, train_dif, test_dif))\n",
    "        epoch_loss.append(cur_epoch_loss)\n",
    "        \n",
    "#         for param in net.parameters():\n",
    "#         #     print(param)\n",
    "#             print(param)\n",
    "        print(torch.max(weight_grad))\n",
    "        print(torch.max(bias_grad))\n",
    "        \n",
    "        if min_test_dif > test_dif:\n",
    "            min_test_dif = test_dif\n",
    "            print('Best')\n",
    "        \n",
    "    return epoch_loss\n",
    "\n",
    "def test_model(model, test_loader, criterion):\n",
    "    # Test the Model\n",
    "    model.eval()\n",
    "    \n",
    "    batch_loss = []\n",
    "    for i, data in enumerate(test_loader):\n",
    "\n",
    "        # get the inputs\n",
    "        x = data['x']\n",
    "        y = data['y']\n",
    "\n",
    "        # x = x.cuda()\n",
    "        # y = y.cuda()\n",
    "\n",
    "        y_hat = net(x)\n",
    "        loss = criterion(y_hat, y)\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "    # Results every epoch\n",
    "    cur_epoch_loss = sum(batch_loss) / len(batch_loss)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return cur_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 512\n",
      "\n",
      "\n",
      "PyTorch - PYNQ backprop RMSE for W:\n",
      "0.000631185\n",
      "PyTorch - PYNQ backprop RMSE for b:\n",
      "0.000137269\n",
      "\n",
      "Pytorch backprop latency:\n",
      "0.0012 s\n",
      "PYNQ backprop latency:\n",
      "0.00048 s\n",
      "Acceleration factor (CPU_Latency / PYNQ_Latency): \n",
      "2.53186\n",
      "\n",
      "Epoch: 0 Train Loss: 66.412178 Train Dif: 66.134560 Test Dif: 57.004086\n",
      "tensor(20.0771)\n",
      "tensor(5.3026)\n",
      "Best\n",
      "\n",
      "\n",
      "PyTorch - PYNQ backprop RMSE for W:\n",
      "0.000578955\n",
      "PyTorch - PYNQ backprop RMSE for b:\n",
      "0.00011917\n",
      "\n",
      "Pytorch backprop latency:\n",
      "0.00131 s\n",
      "PYNQ backprop latency:\n",
      "0.00046 s\n",
      "Acceleration factor (CPU_Latency / PYNQ_Latency): \n",
      "2.828\n",
      "\n",
      "Epoch: 1 Train Loss: 66.134560 Train Dif: 65.858490 Test Dif: 56.792515\n",
      "tensor(19.9928)\n",
      "tensor(5.2794)\n",
      "Best\n",
      "\n",
      "\n",
      "PyTorch - PYNQ backprop RMSE for W:\n",
      "0.000654468\n",
      "PyTorch - PYNQ backprop RMSE for b:\n",
      "0.000150596\n",
      "\n",
      "Pytorch backprop latency:\n",
      "0.00128 s\n",
      "PYNQ backprop latency:\n",
      "0.00046 s\n",
      "Acceleration factor (CPU_Latency / PYNQ_Latency): \n",
      "2.77686\n",
      "\n",
      "Epoch: 2 Train Loss: 65.858490 Train Dif: 65.583984 Test Dif: 56.581882\n",
      "tensor(19.9084)\n",
      "tensor(5.2562)\n",
      "Best\n",
      "\n",
      "\n",
      "PyTorch - PYNQ backprop RMSE for W:\n",
      "0.000608561\n",
      "PyTorch - PYNQ backprop RMSE for b:\n",
      "0.000128029\n",
      "\n",
      "Pytorch backprop latency:\n",
      "0.00129 s\n",
      "PYNQ backprop latency:\n",
      "0.00046 s\n",
      "Acceleration factor (CPU_Latency / PYNQ_Latency): \n",
      "2.81113\n",
      "\n",
      "Epoch: 3 Train Loss: 65.583984 Train Dif: 65.311073 Test Dif: 56.372551\n",
      "tensor(19.8244)\n",
      "tensor(5.2330)\n",
      "Best\n",
      "CPU times: user 7.36 s, sys: 351 ms, total: 7.71 s\n",
      "Wall time: 7.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "\n",
    "overlay = Overlay('/home/xilinx/Linear2x7_112420_2wayDMA_2/backward_lite_features.bit') # Download the bitstream onto the FPGA\n",
    "\n",
    "dma1 = overlay.axi_dma_0 # Backward\n",
    "xlnk = Xlnk() # Used for allocation\n",
    "\n",
    "in_stream = xlnk.cma_array(shape=(32*(3 * 2 + 7),1), dtype=np.float32)\n",
    "out_stream = xlnk.cma_array(shape=((7+1)*2,1), dtype=np.float32)\n",
    "\n",
    "#################### Hyperparameters ####################\n",
    "num_epochs = 4\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0\n",
    "in_frames_num = 3\n",
    "pre_frames_num = 15\n",
    "factor = 0.95\n",
    "patience = 40\n",
    "batch_size = 32\n",
    "#################### Hyperparameters ####################\n",
    "# net = NaiveMLP(in_dim=7, out_dim=2).cuda()\n",
    "# net = NaiveMLP(in_dim=7, out_dim=2)\n",
    "# net = nn.Linear(7,2)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(7, 2),\n",
    "    nn.ReLU(inplace=True)\n",
    "        )\n",
    "criterion = torch.nn.MSELoss()\n",
    "# criterion = torch.nn.functional.smooth_l1_loss\n",
    "\n",
    "train_set = CollisionDataset(\n",
    "    './dataset/dataset/uIsPoint3/train', \n",
    "    sample_num=32\n",
    ")\n",
    "\n",
    "test_set = CollisionDataset(\n",
    "    './dataset/dataset/uIsPoint3/test',\n",
    ")\n",
    "\n",
    "print(len(train_set), len(test_set))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=factor, \n",
    "    patience=patience, \n",
    "    verbose=True, \n",
    "    threshold=1e-3\n",
    ")\n",
    "\n",
    "train_loss = train_model(\n",
    "    net, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    num_epochs, \n",
    "    optimizer, \n",
    "    scheduler, \n",
    "    criterion\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backprop Testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = Overlay('/home/xilinx/Linear2x7_112420_2wayDMA_2/backward_lite_features.bit') # Download the bitstream onto the FPGA\n",
    "\n",
    "# In this accelerator, we are accelerating two kernels and each has its own DMA which are assigned here:\n",
    "dma1 = overlay.axi_dma_0 # Backward\n",
    "# dma2 = overlay.axi_dma_1 # Equation Matrix\n",
    "\n",
    "# Since this IP uses AXI-Lite for the output, we can associate that to a variable and then use our class defined above through \n",
    "# this\n",
    "# backward_ip = overlay.backward_lite_0\n",
    "\n",
    "xlnk = Xlnk() # Used for allocation\n",
    "# Allocating the contiguous arrays of a fixed size:\n",
    "in_stream = xlnk.cma_array(shape=(32*2+32*2+(32*7),1), dtype=np.float32)\n",
    "out_stream = xlnk.cma_array(shape=((7+1)*2,1), dtype=np.float32)\n",
    "\n",
    "in_buffer = xlnk.cma_array(shape=(32,2), dtype=np.float32)\n",
    "out_buffer = xlnk.cma_array(shape=(32,7), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up pytorch network, get x and y data from train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = Overlay('/home/xilinx/Linear2x7_112420_2wayDMA_2/backward_lite_features.bit') # Download the bitstream onto the FPGA\n",
    "\n",
    "# In this accelerator, we are accelerating two kernels and each has its own DMA which are assigned here:\n",
    "dma1 = overlay.axi_dma_0 # Backward\n",
    "# dma2 = overlay.axi_dma_1 # Equation Matrix\n",
    "\n",
    "# Since this IP uses AXI-Lite for the output, we can associate that to a variable and then use our class defined above through \n",
    "# this\n",
    "# backward_ip = overlay.backward_lite_0\n",
    "\n",
    "xlnk = Xlnk() # Used for allocation\n",
    "# Allocating the contiguous arrays of a fixed size:\n",
    "in_stream = xlnk.cma_array(shape=(32*2+32*2+(32*7),1), dtype=np.float32)\n",
    "out_stream = xlnk.cma_array(shape=((7+1)*2,1), dtype=np.float32)\n",
    "\n",
    "in_buffer = xlnk.cma_array(shape=(32,2), dtype=np.float32)\n",
    "out_buffer = xlnk.cma_array(shape=(32,7), dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 512\n"
     ]
    }
   ],
   "source": [
    "#################### Hyperparameters ####################\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0\n",
    "in_frames_num = 3\n",
    "pre_frames_num = 15\n",
    "factor = 0.95\n",
    "patience = 40\n",
    "batch_size = 32\n",
    "#################### Hyperparameters ####################\n",
    "# net = NaiveMLP(in_dim=7, out_dim=2).cuda()\n",
    "# net = NaiveMLP(in_dim=7, out_dim=2)\n",
    "\n",
    "net = nn.Linear(7,2)\n",
    "criterion = torch.nn.MSELoss()\n",
    "# criterion = torch.nn.functional.smooth_l1_loss\n",
    "\n",
    "train_set = CollisionDataset(\n",
    "    './dataset/dataset/uIsPoint3/train', \n",
    "    sample_num=32\n",
    ")\n",
    "\n",
    "test_set = CollisionDataset(\n",
    "    './dataset/dataset/uIsPoint3/test',\n",
    ")\n",
    "\n",
    "print(len(train_set), len(test_set))\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=len(test_set), shuffle=False)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "# optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=factor, \n",
    "    patience=patience, \n",
    "    verbose=True, \n",
    "    threshold=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    # get the inputs\n",
    "    x = data['x']\n",
    "    y = data['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run single forward/backward pass, compare FPGA to CPU results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = Overlay('/home/xilinx/Linear2x7_112420_2wayDMA_2/backward_lite_features.bit') # Download the bitstream onto the FPGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "55.46831130981445\n"
     ]
    }
   ],
   "source": [
    "# In this accelerator, we are accelerating two kernels and each has its own DMA which are assigned here:\n",
    "dma1 = overlay.axi_dma_0 # Backward\n",
    "# dma2 = overlay.axi_dma_1 # Equation Matrix\n",
    "\n",
    "# Since this IP uses AXI-Lite for the output, we can associate that to a variable and then use our class defined above through \n",
    "# this\n",
    "# backward_ip = overlay.backward_lite_0\n",
    "\n",
    "xlnk = Xlnk() # Used for allocation\n",
    "# Allocating the contiguous arrays of a fixed size:\n",
    "in_stream = xlnk.cma_array(shape=(32*(3 * 2 + 7),1), dtype=np.float32)\n",
    "out_stream = xlnk.cma_array(shape=((7+1)*2,1), dtype=np.float32)\n",
    "\n",
    "in_buffer = xlnk.cma_array(shape=(32,2), dtype=np.float32)\n",
    "out_buffer = xlnk.cma_array(shape=(32,7), dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "# model = nn.Linear(7,2)\n",
    "# net = model\n",
    "# model = net\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(7, 2),\n",
    "    nn.ReLU(inplace=True)\n",
    "        )\n",
    "# net = model\n",
    "# model = net\n",
    "\n",
    "# zero the parameter gradients\n",
    "# optimizer.zero_grad()\n",
    "\n",
    "# forward + backward + optimize\n",
    "layer0 = model[0]\n",
    "layer1 = model[1]\n",
    "# z = layer1(x)\n",
    "# y_hat = model(x)\n",
    "\n",
    "# zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "z = layer0(x)\n",
    "y_hat = model(x)\n",
    "\n",
    "loss = criterion(y_hat, y)\n",
    "print('loss')\n",
    "print(loss.item())\n",
    "\n",
    "# Pre-processing for the other accelerator\n",
    "batch_x_stream = x.t()\n",
    "batch_x_stream = batch_x_stream.reshape(32*7, 1)\n",
    "\n",
    "y_stream = y.t()\n",
    "y_stream = y_stream.reshape(32 * 2, 1)\n",
    "y_hat_stream = y_hat.t()\n",
    "y_hat_stream = y_hat_stream.reshape(32 * 2, 1)\n",
    "z_stream = z.t()\n",
    "z_stream = z_stream.reshape(32*2,1)\n",
    "\n",
    "in_stream = xlnk.cma_array(shape=(32*(3*2+7),1), dtype=np.float32)\n",
    "out_stream = xlnk.cma_array(shape=((7+1)*2,1), dtype=np.float32)\n",
    "\n",
    "in_stream_zeros = xlnk.cma_array(shape=(32*(3*2+7),1), dtype=np.float32)\n",
    "in_stream_zeros[:] = np.zeros((32*(3*2+7),1), dtype=np.float32)\n",
    "\n",
    "# print(y[1:10])\n",
    "# print(y_stream)\n",
    "in_stream[:] = torch.cat((y_stream.data, y_hat_stream.data, z_stream.data, batch_x_stream.data), 0).numpy()[:]\n",
    "# print('in_stream')\n",
    "# print(in_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "\n",
    "\n",
    "def backward_manual(y_hat, y, z, x):\n",
    "    diff = y_hat - y\n",
    "\n",
    "\n",
    "    diff = diff * 1 / 32\n",
    "    diff[abs(diff) > 1] = diff[abs(diff) > 1] / abs(diff[abs(diff) > 1])\n",
    "    diff[z < 0] = 0;\n",
    "\n",
    "\n",
    "    # print(diff)\n",
    "\n",
    "    rowIndex = 0\n",
    "    colIndex = 0\n",
    "\n",
    "    manualGrad = np.zeros((2,7))\n",
    "\n",
    "\n",
    "    for row in diff:\n",
    "#         print(manualGrad[0,0])\n",
    "#         print(x[rowIndex][0])\n",
    "#         print(diff[rowIndex][0])\n",
    "#         print(1/32 * (y_hat[rowIndex][0] - y[rowIndex][0]) / (-1 * ))\n",
    "        \n",
    "        for i in range(2):\n",
    "            for j in range(7):\n",
    "                manualGrad[i,j] = manualGrad[i,j] + row[i] * x[rowIndex][j]\n",
    "                \n",
    "        rowIndex = rowIndex + 1\n",
    "    return torch.tensor(manualGrad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch - PYNQ backprop RMSE for W:\n",
      "0.00047887\n",
      "PyTorch - PYNQ backprop RMSE for b:\n",
      "7.3935e-05\n",
      "\n",
      "Pytorch backprop latency:\n",
      "0.00252 s\n",
      "PYNQ backprop latency:\n",
      "0.00219 s\n",
      "Acceleration factor (CPU_Latency / PYNQ_Latency): \n",
      "1.14872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare backprop results\n",
    "t = time.time()\n",
    "loss.backward()\n",
    "pytorchLatency = time.time() - t\n",
    "\n",
    "\n",
    "# Transfer data to the DMA\n",
    "t = time.time()\n",
    "dma1.sendchannel.transfer(in_stream)\n",
    "dma1.recvchannel.transfer(out_stream)\n",
    "dma1.sendchannel.wait()\n",
    "dma1.recvchannel.wait()\n",
    "PYNQLatency = time.time() - t\n",
    "\n",
    "weight_grad = torch.reshape(torch.tensor(out_stream[0:14]), (2,7))\n",
    "\n",
    "bias_grad = torch.reshape(torch.tensor(out_stream[14:16]), (1,2))\n",
    "\n",
    "# manualGrad = backward_manual(y_hat, y, z, x)\n",
    "\n",
    "flag = True\n",
    "for param in model.parameters():\n",
    "    if (flag):\n",
    "        RMSE = np.sqrt(np.mean(np.square((weight_grad - param.grad).numpy())))\n",
    "        print('PyTorch - PYNQ backprop RMSE for W:')\n",
    "        print(RMSE)\n",
    "        flag = False\n",
    "    else:\n",
    "        RMSE = np.sqrt(np.mean(np.square((bias_grad - param.grad).numpy())))\n",
    "        print('PyTorch - PYNQ backprop RMSE for b:')\n",
    "        print(RMSE)\n",
    "        flag = True\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "print('Pytorch backprop latency:')\n",
    "print(str(round(pytorchLatency,5)) + \" s\")\n",
    "print('PYNQ backprop latency:')\n",
    "print(str(round(PYNQLatency,5)) + \" s\")\n",
    "print('Acceleration factor (CPU_Latency / PYNQ_Latency): ')\n",
    "print(round(pytorchLatency / PYNQLatency,5))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7x7 --> 7x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "80.36248016357422\n"
     ]
    }
   ],
   "source": [
    "overlay = Overlay('/home/xilinx/Linear2x7_7x7_120220_2wayDMA/backward_lite_features.bit') # Download the bitstream onto the FPGA\n",
    "\n",
    "dma1 = overlay.axi_dma_0 # Backward\n",
    "xlnk = Xlnk() # Used for allocation\n",
    "\n",
    "N_in = 7\n",
    "N_hidden_0 = 7 \n",
    "N_out = 2\n",
    "\n",
    "# model = nn.Linear(7,2)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(N_in, N_hidden_0),\n",
    "    nn.Linear(N_hidden_0, N_out)\n",
    "        )\n",
    "net = model\n",
    "\n",
    "# zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# forward + backward + optimize\n",
    "layer0 = model[0]\n",
    "layer1 = model[1]\n",
    "\n",
    "z = layer0(x)\n",
    "y_hat = model(x)\n",
    "\n",
    "# print('y_hat')\n",
    "# print(y_hat)\n",
    "\n",
    "loss = criterion(y_hat, y)\n",
    "print('loss')\n",
    "print(loss.item())\n",
    "\n",
    "t = time.time()\n",
    "loss.backward()\n",
    "pytorchLatency = time.time() - t\n",
    "\n",
    "# Merge y_hat, y, z, x, weights into in_stream vector\n",
    "\n",
    "y_stream = y.t()\n",
    "y_stream = y_stream.reshape(32 * 2, 1)\n",
    "\n",
    "y_hat_stream = y_hat.t()\n",
    "y_hat_stream = y_hat_stream.reshape(32 * 2, 1)\n",
    "\n",
    "z_stream = z.t()\n",
    "z_stream = z_stream.reshape(32*7,1)\n",
    "\n",
    "w = layer1.weight\n",
    "w_stream = w.t()\n",
    "w_stream = w_stream.reshape(2*7,1)\n",
    "\n",
    "batch_x_stream = x.t()\n",
    "batch_x_stream = batch_x_stream.reshape(32*7, 1)\n",
    "\n",
    "# Instream [y[32x2] y_hat[32x2] z[32x7] w[2x7] x[32x7]]\n",
    "in_stream = xlnk.cma_array(shape=(32*(2+2+7+7) + 2*7, 1), dtype=np.float32)\n",
    "# Outstream [W_2_7[14] b_2_7[2] W_7_7[49] b_7_7[7]]\n",
    "out_stream = xlnk.cma_array(shape=((7+1)*2+(7+1)*7,1), dtype=np.float32)\n",
    "\n",
    "# print(y[1:10])\n",
    "# print(y_stream)\n",
    "in_stream[:] = torch.cat((y_stream.data, y_hat_stream.data, z_stream.data, batch_x_stream.data, w_stream.data), 0).numpy()[:]\n",
    "# print('in_stream')\n",
    "# print(str(in_stream).replace('[','').replace(']',','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(w)\n",
    "# print(w_stream)\n",
    "\n",
    "# print(w[1,2])\n",
    "# print(w_stream[2 * 2 + 1])\n",
    "\n",
    "# print(z[2,1])\n",
    "# print(z_stream[2 + 32 * 1])\n",
    "\n",
    "# print(x[2,1])\n",
    "# print(batch_x_stream[2 + 32 * 1])\n",
    "\n",
    "# # x[i,j] = x_stream[i + j * height(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "\n",
    "def backward_manual_noReLU(y_hat, y, x):\n",
    "    diff = y_hat - y\n",
    "\n",
    "\n",
    "    diff = diff * 1 / 32\n",
    "    diff[abs(diff) > 1] = diff[abs(diff) > 1] / abs(diff[abs(diff) > 1])\n",
    "#     diff[z < 0] = 0;\n",
    "\n",
    "\n",
    "    # print(diff)\n",
    "\n",
    "    rowIndex = 0\n",
    "    colIndex = 0\n",
    "\n",
    "    manualGrad = np.zeros((2,7))\n",
    "\n",
    "\n",
    "    for row in diff:\n",
    "#         print(manualGrad[0,0])\n",
    "#         print(x[rowIndex][0])\n",
    "#         print(diff[rowIndex][0])\n",
    "#         print(1/32 * (y_hat[rowIndex][0] - y[rowIndex][0]) / (-1 * ))\n",
    "        \n",
    "        for i in range(2):\n",
    "            for j in range(7):\n",
    "                manualGrad[i,j] = manualGrad[i,j] + row[i] * x[rowIndex][j]\n",
    "                \n",
    "        rowIndex = rowIndex + 1\n",
    "    return torch.tensor(manualGrad), diff\n",
    "\n",
    "def backward_manual_2_7_7_7(y_hat, y, z, x, w):\n",
    "    diff = y_hat - y\n",
    "\n",
    "    diff = diff * 1 / 32\n",
    "    diff[abs(diff) > 1] = diff[abs(diff) > 1] / abs(diff[abs(diff) > 1])\n",
    "#     diff[z < 0] = 0;\n",
    "    \n",
    "#     diff = diff * 1 / 32\n",
    "#     diff[abs(diff) > 1] = diff[abs(diff) > 1] / abs(diff[abs(diff) > 1])\n",
    "\n",
    "#     print(diff)\n",
    "#     print(z)\n",
    "\n",
    "    rowIndex = 0\n",
    "    colIndex = 0\n",
    "\n",
    "    manualGrad1 = np.zeros((2,7))\n",
    "    dh = np.zeros((2,7))\n",
    "    manualGrad0 = np.zeros((7,7))\n",
    "\n",
    "    for row in diff:\n",
    "        for i in range(2):\n",
    "            for j in range(7):\n",
    "                manualGrad1[i,j] = manualGrad1[i,j] + row[i] * z[rowIndex][j]\n",
    "                for k in range(7):\n",
    "                    manualGrad0[j,k] = manualGrad0[j,k] + row[i] * w[i][j] * x[rowIndex][k]\n",
    "        rowIndex = rowIndex + 1\n",
    "        \n",
    "    return [torch.tensor(manualGrad0), torch.tensor(manualGrad1), diff]\n",
    "\n",
    "# weights = layer1.weight\n",
    "# print(weights)\n",
    "\n",
    "manualGrad = backward_manual_2_7_7_7(y_hat, y, z, x, w)\n",
    "manualGrad_1layer = backward_manual_noReLU(y_hat, y, z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch - PYNQ backprop RMSE for W 0 :\n",
      "0.000497502\n",
      "PyTorch - PYNQ backprop RMSE for b 0 :\n",
      "0.000530824\n",
      "PyTorch - PYNQ backprop RMSE for W 1 :\n",
      "16.7313\n",
      "PyTorch - PYNQ backprop RMSE for b 1 :\n",
      "0.000223968\n",
      "\n",
      "Pytorch backprop latency:\n",
      "0.00323 s\n",
      "PYNQ backprop latency:\n",
      "0.00204 s\n",
      "Acceleration factor (CPU_Latency / PYNQ_Latency): \n",
      "1.58618\n"
     ]
    }
   ],
   "source": [
    "# Compare backprop results\n",
    "# loss.backward()\n",
    "\n",
    "# manualGrad = backward_manual(y_hat, y, z, x)\n",
    "\n",
    "# Transfer data to the DMA\n",
    "t = time.time()\n",
    "dma1.sendchannel.transfer(in_stream)\n",
    "dma1.recvchannel.transfer(out_stream)\n",
    "dma1.sendchannel.wait()\n",
    "dma1.recvchannel.wait()\n",
    "PYNQLatency = time.time() - t\n",
    "\n",
    "# Obtaining the output from the AXI-Lite interface, as well as post-processing\n",
    "#             bias_grad = torch.tensor([backward_ip.bias1, backward_ip.bias2])\n",
    "#             weight_grad = torch.tensor([[backward_ip.w1_1, backward_ip.w1_2, backward_ip.w1_3, backward_ip.w1_4, backward_ip.w1_5, backward_ip.w1_6, backward_ip.w1_7], \n",
    "#                                         [backward_ip.w2_1, backward_ip.w2_2, backward_ip.w2_3, backward_ip.w2_4, backward_ip.w2_5, backward_ip.w2_6, backward_ip.w2_7]])\n",
    "\n",
    "# TODO Reshape\n",
    "weight_grad = { }\n",
    "bias_grad = { }\n",
    "weight_grad[1] = torch.reshape(torch.tensor(out_stream[0:14]), (2,7))\n",
    "bias_grad[1] = torch.reshape(torch.tensor(out_stream[14:16]), (1,2))\n",
    "\n",
    "weight_grad[0] = torch.reshape(torch.tensor(out_stream[16:16+49]), (7,7))\n",
    "bias_grad[0] = torch.reshape(torch.tensor(out_stream[16+49:16+49+7]), (1,7))\n",
    "\n",
    "\n",
    "# bias_grad = torch.tensor(out_stream[14:16])\n",
    " \n",
    "i = 0\n",
    "flag = True\n",
    "for param in model.parameters():\n",
    "    if (flag):\n",
    "        RMSE = np.sqrt(np.mean(np.square((weight_grad[i] - param.grad).numpy())))\n",
    "        print('PyTorch - PYNQ backprop RMSE for W', str(i), ':')\n",
    "        print(RMSE)\n",
    "        flag = False\n",
    "    else:\n",
    "        RMSE = np.sqrt(np.mean(np.square((bias_grad[i] - param.grad).numpy())))\n",
    "        print('PyTorch - PYNQ backprop RMSE for b', str(i), ':')\n",
    "        print(RMSE)\n",
    "        flag = True\n",
    "        i = i + 1\n",
    "\n",
    "print()\n",
    "print('Pytorch backprop latency:')\n",
    "print(str(round(pytorchLatency,5)) + \" s\")\n",
    "print('PYNQ backprop latency:')\n",
    "print(str(round(PYNQLatency,5)) + \" s\")\n",
    "print('Acceleration factor (CPU_Latency / PYNQ_Latency): ')\n",
    "print(round(pytorchLatency / PYNQLatency,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7x7 --> ReLU --> 7x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapeStreamList(w):\n",
    "    for i in range(len(w)):\n",
    "        if i == 0:\n",
    "            w_stream = reshapeStream(w[i])\n",
    "        else:\n",
    "            w_stream = torch.cat((w_stream,reshapeStream(w[i])),0)\n",
    "    return w_stream\n",
    "    \n",
    "def reshapeStream(x):\n",
    "    x_stream = x.t()\n",
    "    x_stream = x_stream.reshape(np.size(x.data.numpy()), 1)   \n",
    "    return x_stream\n",
    "        \n",
    "def getNetworkFeatures(model, x):\n",
    "    z = []\n",
    "    a = []\n",
    "    w = []\n",
    "    zIndex = 0\n",
    "    aIndex = 0\n",
    "    reluFlag = 0\n",
    "    for name, layer in model.named_modules():\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            if(zIndex == 0):\n",
    "                z.append(layer(x))\n",
    "            else:\n",
    "                if(reluFlag):\n",
    "                    z.append(layer(a[aIndex-1]))\n",
    "                else:\n",
    "                    z.append(layer(z[zIndex-1]))\n",
    "                w.append(layer.weight)\n",
    "            zIndex = zIndex + 1\n",
    "        if isinstance(layer, torch.nn.ReLU):\n",
    "            reluFlag = 1\n",
    "            a.append(layer(z[zIndex - 1]))\n",
    "            aIndex = aIndex + 1\n",
    "            \n",
    "    z = z[0:len(z) - 1] # Discard last element of z ( = y_hat)\n",
    "    return a, z, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "53.46218490600586\n"
     ]
    }
   ],
   "source": [
    "overlay = Overlay('/home/xilinx/Linear7x7_ReLU_7x2/backward_lite_features.bit') # Download the bitstream onto the FPGA\n",
    "\n",
    "dma1 = overlay.axi_dma_0 # Backward\n",
    "xlnk = Xlnk() # Used for allocation\n",
    "\n",
    "N_in = 7\n",
    "N_hidden_0 = 7 \n",
    "N_out = 2\n",
    "\n",
    "# model = nn.Linear(7,2)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(N_in, N_hidden_0),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(N_hidden_0, N_out)\n",
    "        )\n",
    "net = model\n",
    "\n",
    "# zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# forward + backward + optimize\n",
    "a,z,w = getNetworkFeatures(model, x)\n",
    "\n",
    "\n",
    "a,z,w = getNetworkFeatures(model, x)\n",
    "y_hat = model(x)\n",
    "\n",
    "y_stream = reshapeStream(y)\n",
    "y_hat_stream = reshapeStream(y_hat)\n",
    "x_stream = reshapeStream(x)\n",
    "\n",
    "a_stream = reshapeStreamList(a)\n",
    "z_stream = reshapeStreamList(z)\n",
    "w_stream = reshapeStreamList(w)\n",
    "\n",
    "loss = criterion(y_hat, y)\n",
    "print('loss')\n",
    "print(loss.item())\n",
    "t = time.time()\n",
    "loss.backward()\n",
    "pytorchLatency = time.time() - t\n",
    "\n",
    "in_stream_data = torch.cat((y_stream.data, y_hat_stream.data, a_stream.data, z_stream.data, x_stream.data, w_stream.data), 0).numpy()[:]\n",
    "# print(np.size(in_stream_data))\n",
    "sizeInputData = np.size(in_stream_data)\n",
    "numHiddenLayers = len(w) - 1\n",
    "\n",
    "# Pre-processing for the other accelerator\n",
    "\n",
    "# Instream [y[N_out * BATCH_SIZE] \n",
    "#           y_hat[N_out * BATCH_SIZE]\n",
    "#           a[N_hidden * BATCH_SIZE * numReLULayers] \n",
    "#           z[N_hidden * BATCH_SIZE * numHiddenLayers] \n",
    "#           x[N_input * BATCH_SIZE]\n",
    "#           w[N_hidden * N_hidden * numHiddenLayers + N_hidden * N_out]\n",
    "in_stream = xlnk.cma_array(shape=(sizeInputData,1), dtype=np.float32)\n",
    "# Outstream [W b W b ... W0 b0]\n",
    "out_stream = xlnk.cma_array(shape=((N_hidden_0+1)*N_out + (N_hidden_0+1)*N_hidden_0 * numHiddenLayers + (N_in+1)*N_hidden_0,1), dtype=np.float32)\n",
    "\n",
    "in_stream[:] = torch.cat((y_stream.data, y_hat_stream.data, a_stream.data, z_stream.data, x_stream.data, w_stream.data), 0).numpy()[:]\n",
    "# print('in_stream')\n",
    "# print(str(in_stream).replace('[','').replace(']',','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "\n",
    "def backward_manual_noReLU(y_hat, y, x):\n",
    "    diff = y_hat - y\n",
    "\n",
    "    diff = diff * 1 / 32\n",
    "    diff[abs(diff) > 1] = diff[abs(diff) > 1] / abs(diff[abs(diff) > 1])\n",
    "#     diff[z < 0] = 0;\n",
    "\n",
    "\n",
    "    # print(diff)\n",
    "\n",
    "    rowIndex = 0\n",
    "    colIndex = 0\n",
    "\n",
    "    manualGrad = np.zeros((2,7))\n",
    "\n",
    "    for row in diff:\n",
    "#         print(manualGrad[0,0])\n",
    "#         print(x[rowIndex][0])\n",
    "#         print(diff[rowIndex][0])\n",
    "#         print(1/32 * (y_hat[rowIndex][0] - y[rowIndex][0]) / (-1 * ))\n",
    "        \n",
    "        for i in range(2):\n",
    "            for j in range(7):\n",
    "                manualGrad[i,j] = manualGrad[i,j] + row[i] * x[rowIndex][j]\n",
    "                \n",
    "        rowIndex = rowIndex + 1\n",
    "    return torch.tensor(manualGrad), diff\n",
    "\n",
    "def backward_manual_2_7_ReLU_7_7(y_hat, y, a, z, x, w):\n",
    "    diff = y_hat - y\n",
    "\n",
    "    diff = diff * 1 / 32\n",
    "    diff[abs(diff) > 1] = diff[abs(diff) > 1] / abs(diff[abs(diff) > 1])\n",
    "#     diff[z < 0] = 0;\n",
    "    \n",
    "#     diff = diff * 1 / 32\n",
    "#     diff[abs(diff) > 1] = diff[abs(diff) > 1] / abs(diff[abs(diff) > 1])\n",
    "\n",
    "#     print(diff)\n",
    "#     print(z)\n",
    "\n",
    "    rowIndex = 0\n",
    "    colIndex = 0\n",
    "\n",
    "    manualGrad1 = np.zeros((2,7))\n",
    "    dh = np.zeros((2,7))\n",
    "    manualGrad0 = np.zeros((7,7))\n",
    "#     print(z)\n",
    "#     print(a)\n",
    "    \n",
    "\n",
    "    for row in diff:\n",
    "        for i in range(2):\n",
    "            for j in range(7):\n",
    "                manualGrad1[i,j] = manualGrad1[i,j] + row[i] * a[rowIndex][j]\n",
    "                for k in range(7):\n",
    "                    manualGrad0[j,k] = manualGrad0[j,k] + np.heaviside(z[rowIndex][j].detach(),0) * row[i] * w[i][j] * x[rowIndex][k]\n",
    "                \n",
    "#             for i in range(7):        \n",
    "#                 manualGrad0[i,j] = manualGrad0[i,j] + (row[i] * weights[i,j] * x[rowIndex][j])\n",
    "        rowIndex = rowIndex + 1\n",
    "        \n",
    "                \n",
    "#     for row in diff:\n",
    "#         for i in range(7):\n",
    "#             for j in range(7):\n",
    "#                 manualGrad0[i,j] = manualGrad0[i,j] + (row[i] * weights[j,i] * x[rowIndex][j])\n",
    "#         rowIndex = rowIndex + 1\n",
    "\n",
    "#     dh = np.dot(manualGrad1, weights.detach().numpy())\n",
    "#     print(dh)\n",
    "#     manualGrad0 = np.dot(dh, x.T)\n",
    "    \n",
    "    return [torch.tensor(manualGrad0), torch.tensor(manualGrad1), diff]\n",
    "\n",
    "\n",
    "manualGrad = backward_manual_2_7_ReLU_7_7(y_hat, y, a[0], z[0], x, w[0])\n",
    "manualGrad_1layer = backward_manual_noReLU(y_hat, y, a[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYNQ backprop latency:\n"
     ]
    }
   ],
   "source": [
    "# Transfer data to the DMA\n",
    "\n",
    "t = time.time()\n",
    "dma1.sendchannel.transfer(in_stream)\n",
    "dma1.recvchannel.transfer(out_stream)\n",
    "dma1.sendchannel.wait()\n",
    "dma1.recvchannel.wait()\n",
    "PYNQLatency = time.time() - t\n",
    "# Obtaining the output from the AXI-Lite interface, as well as post-processing\n",
    "#             bias_grad = torch.tensor([backward_ip.bias1, backward_ip.bias2])\n",
    "#             weight_grad = torch.tensor([[backward_ip.w1_1, backward_ip.w1_2, backward_ip.w1_3, backward_ip.w1_4, backward_ip.w1_5, backward_ip.w1_6, backward_ip.w1_7], \n",
    "#                                         [backward_ip.w2_1, backward_ip.w2_2, backward_ip.w2_3, backward_ip.w2_4, backward_ip.w2_5, backward_ip.w2_6, backward_ip.w2_7]])\n",
    "\n",
    "# TODO Reshape\n",
    "weight_grad = { }\n",
    "bias_grad = { }\n",
    "weight_grad[1] = torch.reshape(torch.tensor(out_stream[0:14]), (2,7))\n",
    "\n",
    "bias_grad[1] = torch.reshape(torch.tensor(out_stream[14:16]), (1,2))\n",
    "weight_grad[0] = torch.reshape(torch.tensor(out_stream[16:16+49]), (7,7))\n",
    "\n",
    "bias_grad[0] = torch.reshape(torch.tensor(out_stream[16+49:16+49+7]), (1,7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch - PYNQ backprop RMSE for W 0 :\n",
      "0.000176482\n",
      "PyTorch - PYNQ backprop RMSE for b 0 :\n",
      "0.000171727\n",
      "PyTorch - PYNQ backprop RMSE for W 1 :\n",
      "0.000280094\n",
      "PyTorch - PYNQ backprop RMSE for b 1 :\n",
      "0.000233868\n",
      "\n",
      "Pytorch backprop latency:\n",
      "0.00383 s\n",
      "PYNQ backprop latency:\n",
      "0.00214 s\n",
      "Acceleration factor (CPU_Latency / PYNQ_Latency): \n",
      "1.79145\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "flag = True\n",
    "for param in model.parameters():\n",
    "    if (flag):\n",
    "        RMSE = np.sqrt(np.mean(np.square((weight_grad[i] - param.grad).numpy())))\n",
    "        print('PyTorch - PYNQ backprop RMSE for W', str(i), ':')\n",
    "        print(RMSE)\n",
    "        flag = False\n",
    "    else:\n",
    "        RMSE = np.sqrt(np.mean(np.square((bias_grad[i] - param.grad).numpy())))\n",
    "        print('PyTorch - PYNQ backprop RMSE for b', str(i), ':')\n",
    "        print(RMSE)\n",
    "        flag = True\n",
    "        i = i + 1\n",
    "\n",
    "print()\n",
    "print('Pytorch backprop latency:')\n",
    "print(str(round(pytorchLatency,5)) + \" s\")\n",
    "print('PYNQ backprop latency:')\n",
    "print(str(round(PYNQLatency,5)) + \" s\")\n",
    "print('Acceleration factor (CPU_Latency / PYNQ_Latency): ')\n",
    "print(round(pytorchLatency / PYNQLatency,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7x16 --> ReLU --> 16x16 --> ReLU --> 7x2\n",
    "#### Similar to Naive_MLP: (7x128 --> ReLU --> 128x128 --> ReLU --> 128x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshapeStreamList(w):\n",
    "    for i in range(len(w)):\n",
    "        if i == 0:\n",
    "            w_stream = reshapeStream(w[i])\n",
    "        else:\n",
    "            w_stream = torch.cat((w_stream,reshapeStream(w[i])),0)\n",
    "    return w_stream\n",
    "    \n",
    "def reshapeStream(x):\n",
    "    x_stream = x.t()\n",
    "    x_stream = x_stream.reshape(np.size(x.data.numpy()), 1)   \n",
    "    return x_stream\n",
    "        \n",
    "def getNetworkFeatures(model, x):\n",
    "    z = []\n",
    "    a = []\n",
    "    w = []\n",
    "    zIndex = 0\n",
    "    aIndex = 0\n",
    "    reluFlag = 0\n",
    "    for name, layer in model.named_modules():\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            if(zIndex == 0):\n",
    "                z.append(layer(x))\n",
    "            else:\n",
    "                if(reluFlag):\n",
    "                    z.append(layer(a[aIndex-1]))\n",
    "                else:\n",
    "                    z.append(layer(z[zIndex-1]))\n",
    "                w.append(layer.weight)\n",
    "            zIndex = zIndex + 1\n",
    "        if isinstance(layer, torch.nn.ReLU):\n",
    "            reluFlag = 1\n",
    "            a.append(layer(z[zIndex - 1]))\n",
    "            aIndex = aIndex + 1\n",
    "            \n",
    "    z = z[0:len(z) - 1] # Discard last element of z ( = y_hat)\n",
    "    return a, z, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "\n",
    "def backward_manual_noReLU(y_hat, y, x):\n",
    "    diff = y_hat - y\n",
    "\n",
    "    diff = diff * 1 / 32\n",
    "    diff[abs(diff) > 1] = diff[abs(diff) > 1] / abs(diff[abs(diff) > 1])\n",
    "#     diff[z < 0] = 0;\n",
    "\n",
    "\n",
    "    # print(diff)\n",
    "\n",
    "    rowIndex = 0\n",
    "    colIndex = 0\n",
    "\n",
    "    manualGrad = np.zeros((2,16))\n",
    "\n",
    "    for row in diff:\n",
    "#         print(manualGrad[0,0])\n",
    "#         print(x[rowIndex][0])\n",
    "#         print(diff[rowIndex][0])\n",
    "#         print(1/32 * (y_hat[rowIndex][0] - y[rowIndex][0]) / (-1 * ))\n",
    "        \n",
    "        for i in range(2):\n",
    "            for j in range(16):\n",
    "                manualGrad[i,j] = manualGrad[i,j] + row[i] * x[rowIndex][j]\n",
    "                \n",
    "        rowIndex = rowIndex + 1\n",
    "    return torch.tensor(manualGrad), diff\n",
    "\n",
    "def backward_manual_2x16_ReLU_16x16_ReLU_7x7(in_stream):\n",
    "    N_in = 7 \n",
    "    N_hidden = 16\n",
    "    N_out = 2 \n",
    "    BATCH_SIZE = 32\n",
    "    diff = y_hat - y\n",
    "    numHiddenLayers = 1\n",
    "    numReluLayers = 2\n",
    "                   \n",
    "                   \n",
    "    size_y = BATCH_SIZE * (N_out);\n",
    "    size_a = BATCH_SIZE * (N_hidden * numReluLayers);\n",
    "    size_z = BATCH_SIZE * (N_hidden * (numHiddenLayers+1));\n",
    "    size_x = BATCH_SIZE * (N_in);\n",
    "    size_w = (N_out * N_hidden) + (N_hidden * N_hidden * numHiddenLayers);\n",
    "    \n",
    "    LIM1 = size_y;      \n",
    "    LIM2 = size_y * 2;   \n",
    "    LIM3 = size_y * 2 + size_a;   \n",
    "    LIM4 = size_y * 2 + size_a + size_z;   \n",
    "    LIM5 = size_y * 2 + size_a + size_z + size_x; \n",
    "    LIM6 = size_y * 2 + size_a + size_z + size_x + size_w; \n",
    "       \n",
    "    \n",
    "    LIM2W = (N_out) * (N_hidden)\n",
    "    LIM2b = (N_out) * (N_hidden + 1)\n",
    "\n",
    "    LIM1W = (N_out) * (N_hidden + 1) + (N_hidden) * (N_hidden)\n",
    "    LIM1b = (N_out) * (N_hidden + 1) + (N_hidden) * (N_hidden + 1)\n",
    "\n",
    "    LIM0W = (N_out) * (N_hidden + 1) + (N_hidden) * (N_hidden + 1) \\\n",
    "                                     + (N_hidden) * (N_in)\n",
    "    LIM0b = (N_out) * (N_hidden + 1) + (N_hidden) * (N_hidden + 1) \\\n",
    "                                     + (N_hidden) * (N_in + 1)\n",
    "               \n",
    "    nn_out_mat = in_stream[0:LIM1][:]\n",
    "    batch_y_mat = in_stream[LIM1:LIM2][:]  \n",
    "    batch_a_mat = in_stream[LIM2:LIM3][:]\n",
    "    batch_z_mat = in_stream[LIM3:LIM4][:]\n",
    "    batch_x_mat = in_stream[LIM4:LIM5][:]\n",
    "    batch_w_mat = in_stream[LIM5:LIM6][:]\n",
    "               \n",
    "    dataOut = np.zeros([(N_out) * (N_hidden + 1) + \\\n",
    "                        (N_hidden) * (N_hidden + 1) + \\\n",
    "                        (N_hidden) * (N_in + 1)])\n",
    "               \n",
    "    diff = batch_y_mat - nn_out_mat\n",
    "    diff = diff * 1 / 32\n",
    "    diff[abs(diff) > 1] = diff[abs(diff) > 1] / abs(diff[abs(diff) > 1])\n",
    "\n",
    "    rowIndex = 0\n",
    "    colIndex = 0\n",
    "\n",
    "    manualGrad2 = np.zeros((N_out,N_hidden))\n",
    "    manualGrad1 = np.zeros((N_hidden,N_hidden))\n",
    "    manualGrad0 = np.zeros((N_hidden,N_in))\n",
    "#     print(z)\n",
    "#     print(a)\n",
    "    \n",
    "    dRelu = 0\n",
    "    \n",
    "    \n",
    "#     print(np.size(manualGrad1),0)\n",
    "\n",
    "    for batchIndex in range(BATCH_SIZE):\n",
    "        for i in range(N_out):\n",
    "            for j in range(N_hidden):\n",
    "                manualGrad2[i,j] = manualGrad2[i,j] \\\n",
    "                                   + diff[batchIndex + i * BATCH_SIZE] \\\n",
    "                                   * batch_a_mat[batchIndex + j * BATCH_SIZE + N_hidden * BATCH_SIZE]\n",
    "                for k in range(N_hidden):\n",
    "                    manualGrad1[j,k] = manualGrad1[j,k] \\\n",
    "                                       + np.heaviside(batch_z_mat[batchIndex + j * BATCH_SIZE + BATCH_SIZE * N_hidden],0) \\\n",
    "                                       * diff[batchIndex + i * BATCH_SIZE] \\\n",
    "                                       * batch_w_mat[i + j * N_out + N_hidden * N_hidden] \\\n",
    "                                       * batch_z_mat[batchIndex + k * BATCH_SIZE]\n",
    "\n",
    "                    for k0 in range(N_in):\n",
    "                        manualGrad0[k,k0] = manualGrad0[k,k0] \\\n",
    "                                           + np.heaviside(batch_z_mat[batchIndex + k * BATCH_SIZE],0) \\\n",
    "                                           * diff[batchIndex + i * BATCH_SIZE] \\\n",
    "                                           * np.heaviside(batch_z_mat[batchIndex + j * BATCH_SIZE + BATCH_SIZE * N_hidden],0) \\\n",
    "                                           * batch_w_mat[i + j * N_out + N_hidden * N_hidden] \\\n",
    "                                           * batch_w_mat[j + k * N_hidden] * batch_x_mat[batchIndex + k0 * BATCH_SIZE]\n",
    "    \n",
    "    return [torch.tensor(manualGrad0), torch.tensor(manualGrad1), torch.tensor(manualGrad2), diff]\n",
    "\n",
    "# in_stream_vector = torch.cat((y_stream.data, y_hat_stream.data, a_stream.data, z_stream.data, batch_x_stream.data, w_stream.data), 0).numpy()[:]\n",
    "\n",
    "# manualGrad = backward_manual_2x16_ReLU_16x16_ReLU_7x7(in_stream_vector)\n",
    "# manualGrad_1layer = backward_manual_noReLU(y_hat, y, a[1])\n",
    "\n",
    "# print(manualGrad[0])\n",
    "# print(manualGrad[1])\n",
    "# print(manualGrad[1] - pytorchGrad[1])\n",
    "# print(manualGrad_1layer[0])\n",
    "# print(manualGrad[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "54.51510238647461\n"
     ]
    }
   ],
   "source": [
    "overlay = Overlay('/home/xilinx/Linear7x16_ReLU_16x16_ReLU_16x2/backward_lite_features.bit') # Download the bitstream onto the FPGA\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "dma1 = overlay.axi_dma_0 # Backward\n",
    "xlnk = Xlnk() # Used for allocation\n",
    "\n",
    "N_in = 7\n",
    "N_hidden_0 = 16 \n",
    "N_hidden = N_hidden_0\n",
    "N_out = 2\n",
    "\n",
    "# model = nn.Linear(7,2)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(N_in, N_hidden_0),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(N_hidden_0, N_hidden_0),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(N_hidden_0, N_out)\n",
    "        )\n",
    "net = model\n",
    "\n",
    "# zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "a,z,w = getNetworkFeatures(model, x)\n",
    "y_hat = model(x)\n",
    "\n",
    "y_stream = reshapeStream(y)\n",
    "y_hat_stream = reshapeStream(y_hat)\n",
    "x_stream = reshapeStream(x)\n",
    "\n",
    "a_stream = reshapeStreamList(a)\n",
    "z_stream = reshapeStreamList(z)\n",
    "w_stream = reshapeStreamList(w)\n",
    "\n",
    "loss = criterion(y_hat, y)\n",
    "print('loss')\n",
    "print(loss.item())\n",
    "t = time.time()\n",
    "loss.backward()\n",
    "pytorchLatency = time.time() - t\n",
    "\n",
    "# Store pytorch grad in list\n",
    "pytorchGrad = []\n",
    "flag = True\n",
    "layerNum = 0\n",
    "for param in model.parameters():\n",
    "    if (flag):\n",
    "        pytorchGrad.append(param.grad)\n",
    "        flag = False\n",
    "    else:\n",
    "        flag = True\n",
    "        layerNum = layerNum + 1\n",
    "\n",
    "in_stream_data = torch.cat((y_stream.data, y_hat_stream.data, a_stream.data, z_stream.data, x_stream.data, w_stream.data), 0).numpy()[:]\n",
    "sizeInputData = np.size(in_stream_data)\n",
    "numHiddenLayers = len(w) - 1\n",
    "\n",
    "# Pre-processing for the other accelerator\n",
    "# Instream [y[N_out * BATCH_SIZE] \n",
    "#           y_hat[N_out * BATCH_SIZE]\n",
    "#           a[N_hidden * BATCH_SIZE * numReLULayers] \n",
    "#           z[N_hidden * BATCH_SIZE * numHiddenLayers] \n",
    "#           x[N_input * BATCH_SIZE]\n",
    "#           w[N_hidden * N_hidden * numHiddenLayers + N_hidden * N_out]\n",
    "in_stream = xlnk.cma_array(shape=(sizeInputData,1), dtype=np.float32)\n",
    "\n",
    "# Outstream [W b W b ... W0 b0]\n",
    "out_stream = xlnk.cma_array(shape=((N_hidden_0+1)*N_out + (N_hidden_0+1)*N_hidden_0 * numHiddenLayers + (N_in+1)*N_hidden_0,1), dtype=np.float32)\n",
    "\n",
    "in_stream[:] = torch.cat((y_stream.data, y_hat_stream.data, a_stream.data, z_stream.data, x_stream.data, w_stream.data), 0).numpy()[:]\n",
    "# print('in_stream')\n",
    "# print(str(in_stream).replace('[','').replace(']',','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer data to the DMA\n",
    "\n",
    "t = time.time()\n",
    "dma1.sendchannel.transfer(in_stream)\n",
    "dma1.recvchannel.transfer(out_stream)\n",
    "dma1.sendchannel.wait()\n",
    "dma1.recvchannel.wait()\n",
    "PYNQLatency = time.time() - t\n",
    "\n",
    "# TODO Reshape\n",
    "weight_grad = { }\n",
    "bias_grad = { }\n",
    "\n",
    "LIM2W = N_out * N_hidden;\n",
    "LIM2b = LIM2W + N_out;\n",
    "\n",
    "LIM1W = LIM2b + N_hidden * N_hidden;\n",
    "LIM1b = LIM1W + N_hidden;\n",
    "\n",
    "LIM0W = LIM1b + N_in * N_hidden;\n",
    "LIM0b = LIM0W + N_hidden;\n",
    "\n",
    "weight_grad[2] = torch.reshape(torch.tensor(out_stream[0:LIM2W]), (N_out,N_hidden))\n",
    "bias_grad[2] = torch.reshape(torch.tensor(out_stream[LIM2W:LIM2b]), (1,N_out))\n",
    "\n",
    "weight_grad[1] = torch.reshape(torch.tensor(out_stream[LIM2b:LIM1W]), (N_hidden,N_hidden))\n",
    "bias_grad[1] = torch.reshape(torch.tensor(out_stream[LIM1W:LIM1b]), (1,N_hidden))\n",
    "\n",
    "weight_grad[0] = torch.reshape(torch.tensor(out_stream[LIM1b:LIM0W]), (N_hidden,N_in))\n",
    "bias_grad[0] = torch.reshape(torch.tensor(out_stream[LIM0W:LIM0b]), (1,N_hidden))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch - PYNQ backprop RMSE for W 0 :\n",
      "0.0934093\n",
      "PyTorch - PYNQ backprop RMSE for b 0 :\n",
      "0.0124329\n",
      "PyTorch - PYNQ backprop RMSE for W 1 :\n",
      "0.11091\n",
      "PyTorch - PYNQ backprop RMSE for b 1 :\n",
      "0.0434416\n",
      "PyTorch - PYNQ backprop RMSE for W 2 :\n",
      "0.327921\n",
      "PyTorch - PYNQ backprop RMSE for b 2 :\n",
      "0.223616\n",
      "\n",
      "Pytorch backprop latency:\n",
      "0.00396 s\n",
      "PYNQ backprop latency:\n",
      "0.01155 s\n",
      "Acceleration factor (CPU_Latency / PYNQ_Latency): \n",
      "0.34314\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "flag = True\n",
    "for param in model.parameters():\n",
    "    if (flag):\n",
    "        RMSE = np.sqrt(np.mean(np.square((weight_grad[i] - param.grad).numpy())))\n",
    "        print('PyTorch - PYNQ backprop RMSE for W', str(i), ':')\n",
    "        print(RMSE)\n",
    "        flag = False\n",
    "    else:\n",
    "        RMSE = np.sqrt(np.mean(np.square((bias_grad[i] - param.grad).numpy())))\n",
    "        print('PyTorch - PYNQ backprop RMSE for b', str(i), ':')\n",
    "        print(RMSE)\n",
    "        flag = True\n",
    "        i = i + 1\n",
    "\n",
    "print()\n",
    "print('Pytorch backprop latency:')\n",
    "print(str(round(pytorchLatency,5)) + \" s\")\n",
    "print('PYNQ backprop latency:')\n",
    "print(str(round(PYNQLatency,5)) + \" s\")\n",
    "print('Acceleration factor (CPU_Latency / PYNQ_Latency): ')\n",
    "print(round(pytorchLatency / PYNQLatency,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7x16 --> ReLU --> 16x16 --> ReLU --> 7x2 No Opt\n",
    "#### Similar to Naive_MLP: (7x128 --> ReLU --> 128x128 --> ReLU --> 128x2)\n",
    "### No Loop unrolling or pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "57.34963607788086\n"
     ]
    }
   ],
   "source": [
    "overlay = Overlay('/home/xilinx/Linear7x16_ReLU_16x16_ReLU_16x2_NoOpt/backward_lite_features.bit') # Download the bitstream onto the FPGA\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "dma1 = overlay.axi_dma_0 # Backward\n",
    "xlnk = Xlnk() # Used for allocation\n",
    "\n",
    "N_in = 7\n",
    "N_hidden_0 = 16 \n",
    "N_hidden = N_hidden_0\n",
    "N_out = 2\n",
    "\n",
    "# model = nn.Linear(7,2)\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(N_in, N_hidden_0),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(N_hidden_0, N_hidden_0),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(N_hidden_0, N_out)\n",
    "        )\n",
    "net = model\n",
    "\n",
    "# zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "a,z,w = getNetworkFeatures(model, x)\n",
    "y_hat = model(x)\n",
    "\n",
    "y_stream = reshapeStream(y)\n",
    "y_hat_stream = reshapeStream(y_hat)\n",
    "x_stream = reshapeStream(x)\n",
    "\n",
    "a_stream = reshapeStreamList(a)\n",
    "z_stream = reshapeStreamList(z)\n",
    "w_stream = reshapeStreamList(w)\n",
    "\n",
    "loss = criterion(y_hat, y)\n",
    "print('loss')\n",
    "print(loss.item())\n",
    "t = time.time()\n",
    "loss.backward()\n",
    "pytorchLatency = time.time() - t\n",
    "\n",
    "# Store pytorch grad in list\n",
    "pytorchGrad = []\n",
    "flag = True\n",
    "layerNum = 0\n",
    "for param in model.parameters():\n",
    "    if (flag):\n",
    "        pytorchGrad.append(param.grad)\n",
    "        flag = False\n",
    "    else:\n",
    "        flag = True\n",
    "        layerNum = layerNum + 1\n",
    "\n",
    "in_stream_data = torch.cat((y_stream.data, y_hat_stream.data, a_stream.data, z_stream.data, x_stream.data, w_stream.data), 0).numpy()[:]\n",
    "sizeInputData = np.size(in_stream_data)\n",
    "numHiddenLayers = len(w) - 1\n",
    "\n",
    "# Pre-processing for the other accelerator\n",
    "# Instream [y[N_out * BATCH_SIZE] \n",
    "#           y_hat[N_out * BATCH_SIZE]\n",
    "#           a[N_hidden * BATCH_SIZE * numReLULayers] \n",
    "#           z[N_hidden * BATCH_SIZE * numHiddenLayers] \n",
    "#           x[N_input * BATCH_SIZE]\n",
    "#           w[N_hidden * N_hidden * numHiddenLayers + N_hidden * N_out]\n",
    "in_stream = xlnk.cma_array(shape=(sizeInputData,1), dtype=np.float32)\n",
    "\n",
    "# Outstream [W b W b ... W0 b0]\n",
    "out_stream = xlnk.cma_array(shape=((N_hidden_0+1)*N_out + (N_hidden_0+1)*N_hidden_0 * numHiddenLayers + (N_in+1)*N_hidden_0,1), dtype=np.float32)\n",
    "\n",
    "in_stream[:] = torch.cat((y_stream.data, y_hat_stream.data, a_stream.data, z_stream.data, x_stream.data, w_stream.data), 0).numpy()[:]\n",
    "# print('in_stream')\n",
    "# print(str(in_stream).replace('[','').replace(']',','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer data to the DMA\n",
    "\n",
    "t = time.time()\n",
    "dma1.sendchannel.transfer(in_stream)\n",
    "dma1.recvchannel.transfer(out_stream)\n",
    "dma1.sendchannel.wait()\n",
    "dma1.recvchannel.wait()\n",
    "PYNQLatency = time.time() - t\n",
    "\n",
    "# TODO Reshape\n",
    "weight_grad = { }\n",
    "bias_grad = { }\n",
    "\n",
    "LIM2W = N_out * N_hidden;\n",
    "LIM2b = LIM2W + N_out;\n",
    "\n",
    "LIM1W = LIM2b + N_hidden * N_hidden;\n",
    "LIM1b = LIM1W + N_hidden;\n",
    "\n",
    "LIM0W = LIM1b + N_in * N_hidden;\n",
    "LIM0b = LIM0W + N_hidden;\n",
    "\n",
    "weight_grad[2] = torch.reshape(torch.tensor(out_stream[0:LIM2W]), (N_out,N_hidden))\n",
    "bias_grad[2] = torch.reshape(torch.tensor(out_stream[LIM2W:LIM2b]), (1,N_out))\n",
    "\n",
    "weight_grad[1] = torch.reshape(torch.tensor(out_stream[LIM2b:LIM1W]), (N_hidden,N_hidden))\n",
    "bias_grad[1] = torch.reshape(torch.tensor(out_stream[LIM1W:LIM1b]), (1,N_hidden))\n",
    "\n",
    "weight_grad[0] = torch.reshape(torch.tensor(out_stream[LIM1b:LIM0W]), (N_hidden,N_in))\n",
    "bias_grad[0] = torch.reshape(torch.tensor(out_stream[LIM0W:LIM0b]), (1,N_hidden))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch - PYNQ backprop RMSE for W 0 :\n",
      "0.0740186\n",
      "PyTorch - PYNQ backprop RMSE for b 0 :\n",
      "0.0131191\n",
      "PyTorch - PYNQ backprop RMSE for W 1 :\n",
      "0.119391\n",
      "PyTorch - PYNQ backprop RMSE for b 1 :\n",
      "0.0404381\n",
      "PyTorch - PYNQ backprop RMSE for W 2 :\n",
      "0.411409\n",
      "PyTorch - PYNQ backprop RMSE for b 2 :\n",
      "0.353297\n",
      "\n",
      "Pytorch backprop latency:\n",
      "0.00377 s\n",
      "PYNQ backprop latency:\n",
      "0.01141 s\n",
      "Acceleration factor (CPU_Latency / PYNQ_Latency): \n",
      "0.33003\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "flag = True\n",
    "for param in model.parameters():\n",
    "    if (flag):\n",
    "        RMSE = np.sqrt(np.mean(np.square((weight_grad[i] - param.grad).numpy())))\n",
    "        print('PyTorch - PYNQ backprop RMSE for W', str(i), ':')\n",
    "        print(RMSE)\n",
    "        flag = False\n",
    "    else:\n",
    "        RMSE = np.sqrt(np.mean(np.square((bias_grad[i] - param.grad).numpy())))\n",
    "        print('PyTorch - PYNQ backprop RMSE for b', str(i), ':')\n",
    "        print(RMSE)\n",
    "        flag = True\n",
    "        i = i + 1\n",
    "\n",
    "print()\n",
    "print('Pytorch backprop latency:')\n",
    "print(str(round(pytorchLatency,5)) + \" s\")\n",
    "print('PYNQ backprop latency:')\n",
    "print(str(round(PYNQLatency,5)) + \" s\")\n",
    "print('Acceleration factor (CPU_Latency / PYNQ_Latency): ')\n",
    "print(round(pytorchLatency / PYNQLatency,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
